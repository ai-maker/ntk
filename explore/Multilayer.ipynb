{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Multilayer Perceptron\n",
      "\n",
      "Some say that 9 out of 10 people who use neural networks apply a Multilayer Perceptron (MLP). A MLP is basically a feed-forward network with 3 layers (at least): an input layer, an output layer, and a hidden layer in between. Thus, the MLP has no structural loops: information always flows from left (input)to right (output). The lack of inherent feedback saves a lot of headaches. Its analysis is totally straightforward given that the output of the network is always a function of the input, it does not depend on any former state of the model or previous input.\n",
      "\n",
      "Regarding the topology of a MLP it is normally assumed to be a densely-meshed one-to-many link model between the layers. This is mathematically represented by two matrices of parameters named \u201cthe thetas\u201d. In any case, if a certain connection is of little relevance with respect to the observable training data, the network will automatically pay little attention to its contribution and assign it a low weight close to zero.\n",
      "\n",
      "## Prediction\n",
      "\n",
      "The evaluation of the output of a MLP, i.e., its prediction, given an input vector of data is a matter of matrix multiplication. To that end, the following variables are described for convenience:\n",
      "* $N$ is the dimension of the input layer.\n",
      "* $H$ is the dimension of the hidden layer.\n",
      "* $K$ is the dimension of the output layer.\n",
      "* $M$ is the dimension of the corpus (number of examples).\n",
      "\n",
      "Given the variables above, the parameters of the network, i.e., the thetas matrices, are defined as follows:\n",
      "* $\\theta^{(IN)} \\rightarrow H \\times (N+1)$\n",
      "* $\\theta^{(OUT)} \\rightarrow K \\times (H+1)$\n",
      "\n",
      "The thetas are randomly initialised."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import NeuralNetwork\n",
      "\n",
      "nn = NeuralNetwork.Multilayer([2,3,1])\n",
      "\n",
      "# nn[0] -> ThetaIN, nn[1] -> ThetaOUT\n",
      "print(nn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[array([[-0.37495217, -0.10792651,  0.3473243 ],\n",
        "       [ 0.9814929 ,  0.82317322, -0.46427325],\n",
        "       [ 0.03868529, -0.69017205,  0.71859997]]), array([[-0.38964549, -0.99765327, -0.02448981, -0.28826879]])]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What follows are the ordered steps that need to be followed in order to evaluate the network prediction.\n",
      "\n",
      "### Input Feature Expansion\n",
      "\n",
      "The first step to attain a successful operation of the neural network is to add a bias term to the input feature space (mapped to the input layer):\n",
      "\n",
      "$$a^{(IN)} = [1;\\ x]$$\n",
      "\n",
      "The feature expansion of the input space with the bias term increases the learning effectiveness of the model because it adds a degree of freedom to the adaptation process. Note that $a^{(IN)}$ directly represents the activation values of the input layer. Thus, the input layer is linear with the input vector $x$ (it is defined by a linear activation function).\n",
      "\n",
      "### Transit to the Hidden Layer\n",
      "\n",
      "Once the activations (outputs) of the input layer are determined, their values flow into the hidden layer through the weights defined in $\\theta^{(IN)}$:\n",
      "\n",
      "$$z^{(HID)} = \\theta^{(IN)}\\;a^{(IN)}$$\n",
      "\n",
      "Similarly, the dimensionality of the hidden layer is expanded with a bias term to increase its learning effectiveness:\n",
      "\n",
      "$$a^{(HID)} = [1;\\ g(z^{(HID)})]$$\n",
      "\n",
      "Here, a new function $g()$ is introduced. This is the generic activation function of a neuron, and generally it is non-linear. Its application yields the output values of the hidden layer $a^{(HID)}$ and provides the true learning power to the neural model.\n",
      "\n",
      "### Output\n",
      "\n",
      "Finally, the activation values of the output layer, i.e., the network prediction, are calculated as follows:\n",
      "\n",
      "$$z^{(OUT)} = \\theta^{(OUT)}\\;a^{(HID)}$$\n",
      "\n",
      "$$a^{(OUT)} = g(z^{(OUT)}) = y$$\n",
      "\n",
      "### Activation Function\n",
      "\n",
      "The activation function of the neuron is (usually) a non-linear function that provides the expressive power to the neural network. It is recommended this function to be smooth, differentiable and monotonically non-decreasing (for learning purposes). Typically, the sigmoid function is used.\n",
      "\n",
      "$$g(z) = \\frac{1}{(1 + \\exp^{-z})}$$\n",
      "\n",
      "Note that the range of this function varies from 0 to 1. Therefore, the output values of the neurons will always be bounded by the upper and the lower limits of this range. This entails considering a scaling process if a broader range of predicted values is needed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}