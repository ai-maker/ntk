{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Multilayer Perceptron\n",
      "\n",
      "Some say that 9 out of 10 people who use neural networks apply a Multilayer Perceptron (MLP). A MLP is basically a feed-forward network with 3 layers (at least): an input layer, an output layer, and a hidden layer in between. Thus, the MLP has no structural loops: information always flows from left (input)to right (output). The lack of inherent feedback saves a lot of headaches. Its analysis is totally straightforward given that the output of the network is always a function of the input, it does not depend on any former state of the model or previous input.\n",
      "\n",
      "![Multilayer Perceptron topology](files/multilayer.png)\n",
      "\n",
      "Regarding the topology of a MLP it is normally assumed to be a densely-meshed one-to-many link model between the layers. This is mathematically represented by two matrices of parameters named \u201cthe thetas\u201d. In any case, if a certain connection is of little relevance with respect to the observable training data, the network will automatically pay little attention to its contribution and assign it a low weight close to zero.\n",
      "\n",
      "## Prediction\n",
      "\n",
      "The evaluation of the output of a MLP, i.e., its prediction, given an input vector of data is a matter of matrix multiplication. To that end, the following variables are described for convenience:\n",
      "* $N$ is the dimension of the input layer.\n",
      "* $H$ is the dimension of the hidden layer.\n",
      "* $K$ is the dimension of the output layer.\n",
      "* $M$ is the dimension of the corpus (number of examples).\n",
      "\n",
      "Given the variables above, the parameters of the network, i.e., the thetas matrices, are defined as follows:\n",
      "* $\\theta^{(IN)} \\rightarrow H \\times (N+1)$\n",
      "* $\\theta^{(OUT)} \\rightarrow K \\times (H+1)$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import NeuralNetwork\n",
      "\n",
      "# 2 input neurons, 3 hidden neurons, 1 output neuron\n",
      "nn = NeuralNetwork.Multilayer([2,3,1])\n",
      "\n",
      "# nn[0] -> ThetaIN, nn[1] -> ThetaOUT\n",
      "print(nn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[array([[ 0.80008748, -0.05186959, -0.53827924],\n",
        "       [ 0.00970163,  0.24758429, -0.76940542],\n",
        "       [ 0.59404117,  0.18045765,  0.81416911]]), array([[-0.49574824,  0.92862925,  0.49319575, -1.04762809]])]\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What follows are the ordered steps that need to be followed in order to evaluate the network prediction.\n",
      "\n",
      "### Input Feature Expansion\n",
      "\n",
      "The first step to attain a successful operation of the neural network is to add a bias term to the input feature space (mapped to the input layer):\n",
      "\n",
      "$$a^{(IN)} = [1;\\ x]$$\n",
      "\n",
      "The feature expansion of the input space with the bias term increases the learning effectiveness of the model because it adds a degree of freedom to the adaptation process. Note that $a^{(IN)}$ directly represents the activation values of the input layer. Thus, the input layer is linear with the input vector $x$ (it is defined by a linear activation function).\n",
      "\n",
      "### Transit to the Hidden Layer\n",
      "\n",
      "Once the activations (outputs) of the input layer are determined, their values flow into the hidden layer through the weights defined in $\\theta^{(IN)}$:\n",
      "\n",
      "$$z^{(HID)} = \\theta^{(IN)}\\;a^{(IN)}$$\n",
      "\n",
      "Similarly, the dimensionality of the hidden layer is expanded with a bias term to increase its learning effectiveness:\n",
      "\n",
      "$$a^{(HID)} = [1;\\ g(z^{(HID)})]$$\n",
      "\n",
      "Here, a new function $g()$ is introduced. This is the generic activation function of a neuron, and generally it is non-linear. Its application yields the output values of the hidden layer $a^{(HID)}$ and provides the true learning power to the neural model.\n",
      "\n",
      "### Output\n",
      "\n",
      "Finally, the activation values of the output layer, i.e., the network prediction, are calculated as follows:\n",
      "\n",
      "$$z^{(OUT)} = \\theta^{(OUT)}\\;a^{(HID)}$$\n",
      "\n",
      "and finally\n",
      "\n",
      "$$a^{(OUT)} = g(z^{(OUT)}) = y$$\n",
      "\n",
      "### Activation Function\n",
      "\n",
      "The activation function of the neuron is (usually) a non-linear function that provides the expressive power to the neural network. It is recommended this function to be smooth, differentiable and monotonically non-decreasing (for learning purposes). Typically, the logistic sigmoid function is used.\n",
      "\n",
      "$$g(z) = \\frac{1}{(1 + \\exp^{-z})}$$\n",
      "\n",
      "Note that the range of this function varies from 0 to 1. Therefore, the output values of the neurons will always be bounded by the upper and the lower limits of this range. This entails considering a scaling process if a broader range of predicted values is needed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "# Random instance with 2 values\n",
      "x = np.array([1.0, 2.0])\n",
      "y = NeuralNetwork.Predict(nn, x)\n",
      "\n",
      "# intermediate results are available\n",
      "# y[0] -> input result, y[1] -> hidden result, y[2] -> output result\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[array([ 1.,  2.]), array([ 0.41864444,  0.21729076,  0.91704339]), array([ 0.27677083])]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z = np.arange(-8, 8, 0.1)\n",
      "g = NeuralNetwork.Sigmoid(z)\n",
      "\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(z, g, 'b-', label='g(z)')\n",
      "plt.legend(loc='upper left')\n",
      "plt.xlabel('Input [z]')\n",
      "plt.ylabel('Output [g]')\n",
      "plt.title('Logistic sigmoid activation function')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEZCAYAAABxbJkKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0xb105930c>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Training\n",
      "\n",
      "Training a neural network essentially means fitting its parameters to a set of example data considering an objective function, aka cost function. This process is also known as supervised learning. It is usually implemented as an iterative procedure.\n",
      "\n",
      "### Cost Function\n",
      "\n",
      "The cost function somehow encodes the objective or goal that should be attained with the network. It is usually defined as a classification or a regression evaluation function. However, the actual form of the cost function is effectively the same, which is an error or fitting function.\n",
      "\n",
      "The cost function $J$ quantifies the amount of squared error (or misfitting) that the network displays with respect to a set of data. Thus, in order to achieve a successfully working model, this cost function must be minimised with an adequate set of parameter values. To do so, several solutions are valid as long as this cost function be a convex function (i.e., a bowl-like shape). A well known example of such is the quadratic function, which trains the neural network considering a minimum squared error criterion over the whole dataset of training examples:\n",
      "\n",
      "$$J(\\theta, x) = \\frac{1}{M} \\sum_{m=1}^M \\sum_{k=1}^K \\left(Error_k^{(m)}\\right)^2 = \\frac{1}{M} \\sum_{m=1}^M \\sum_{k=1}^K \\left(t_k^{(m)}-y_k^{(m)}\\right)^2$$\n",
      "\n",
      "Note that the term $t$ in the cost function represents the target value of the network (i.e., the ideal/desired network output) for a given input data value $x$. Now that the cost function can be expressed, a convex optimisation procedure (e.g., a gradient-based method) must be conducted in order to minimise its value. Note that this is essentially a least-squares regression.\n",
      "\n",
      "### Regularisation\n",
      "\n",
      "The mean squared-error cost function described above does not incorporate any knowledge or constraint about the characteristics of the parameters being adjusted through the optimisation training strategy. This may develop into a generalisation problem because the space of solutions is large and some of these solutions may turn the model unstable with new unseen data. Therefore, there is the need to smooth the performance of the model over a wide range of input data.\n",
      "\n",
      "Neural networks usually generalise well as long as the weights are kept small. Thus, the Tikhonov regularisation process, aka ridge regression, is introduced as a means to control complexity of the model in favour of its increased general performance. This regularisation approach favours small weight values (it is a cost over large weight values):\n",
      "\n",
      "$$R(\\theta) = \\frac{\\lambda}{2 M} \\sum_{\\forall \\theta \\notin bias} \\theta^2$$\n",
      "\n",
      "There is a typical trade-off in Machine Learning, known as the bias-variance trade-off, which has a direct relationship with the complexity of the model, the nature of the data and the amount of available training data to adjust it. This ability of the model to learn more or less complex scenarios raises an issue with respect to its fitting: if the data is simple to explain, a complex model is said to overfit the data, causing its overall performance to drop (high variance model). Similarly, if complex data is tackled with a simple model, such model is said to underfit the data, also causing its overall performance to drop (high bias model). As it is usual in engineering, a compromise must be reached with an adequate \u03bb\\lambda\u03bb value.\n",
      "\n",
      "### Parameter Initialisation\n",
      "\n",
      "The initial weights of the thetas assigned by the training process are critical with respect to the success of the learning strategy. They determine the starting point of the optimisation procedure, and depending on their value, the adjusted parameter values may end up in different places if the cost function has multiple (local) minima.\n",
      "\n",
      "The parameter initialisation process is based on a uniform distribution between two small numbers that take into account the amount of input and output units of the adjacent layers:\n",
      "\n",
      "$$\\theta_{init} = U[-\\sigma, +\\sigma]\\ \\ where\\ \\ \\sigma = \\frac{\\sqrt{6}}{\\sqrt{in + out}}$$\n",
      "\n",
      "In order to ensure a proper learning procedure, the weights of the parameters need to be randomly assigned in order to prevent any symmetry in the topology of the network model (that would be likely to end in convergence problems).\n",
      "\n",
      "### Gradient Descent\n",
      "\n",
      "Given the convex shape of the cost function (which usually also includes the regularisation), the minimisation objective boils down to finding the extremum of this function using its derivative in the continuos space of the weights. To this end you may use the analytic form of the derivative of the cost function (a nightmare), a numerical finite difference, or automatic differentiation.\n",
      "\n",
      "Gradient descent is a first-order optimisation algorithm, complete but non-optimal. It first starts with some arbitrarily chosen parameters and computes the derivative of the cost function with respect to each of them $\\frac{\\partial J(\\theta,x)}{\\partial \\theta}$. The model parameters are then updated by moving them some distance (determined by the so called learning rate $\\eta$) from the former initial point in the direction of the steepest descent, i.e., along the negative of the gradient. If $\\eta$ is set too small, though, convergence is needlessly slow, whereas if it is too large, the update correction process may overshoot and even diverge.\n",
      "\n",
      "$$\\theta^{t+1} \\leftarrow \\theta^t - \\eta \\frac{\\partial^t J(\\theta,x)}{\\partial \\theta} $$\n",
      "\n",
      "These steps are iterated in a loop until some stopping criterion is met, e.g., a determined number of epochs (i.e., the single presentation of all patterns in the training example set) is reached. One last remark should be made about the amount of examples $M$ used for learning. If the training procedure considers several instances at once per cost computation and parameter update, i.e., $M \\gg 1$, the approach is called batch learning. Batch learning is slow because each cost computation accounts for all the available training instances. In contrast, it is usual to consider only one training instance at a time, i.e., $M=1$, in order to speed up the iterative learning process. This procedure is called online learning. Online learning steps are faster to compute, but this single-instance approximation of the cost function makes it a little inaccurate around the optimum. However, online learning is very convenient in most cases."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load Iris dataset, 4 input, 1 output\n",
      "from sklearn import datasets as dset\n",
      "\n",
      "iris = dset.load_iris()\n",
      "\n",
      "nn = NeuralNetwork.Multilayer([4,4,3])\n",
      "# Target needs to be divided by 3 because of the sigmoid,\n",
      "# regularisation parameter of 0.2\n",
      "tcost = NeuralNetwork.Cost(nn, iris.data, iris.target/3, 0.2)\n",
      "\n",
      "# Cost value for an untrained network\n",
      "print(\"J(ini) = \" + str(tcost))\n",
      "\n",
      "# Train with numerical gradient, 20 rounds\n",
      "# learning rate is 0.1\n",
      "NeuralNetwork.NumGradDesc(nn, iris.data, iris.target/3, 0.2, 20, 0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "J(ini) = 1.30532441436\n",
        "J(0) = 0.940783407986"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(1) = 0.637383865144"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(2) = 0.421782711725"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(3) = 0.290234921005"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(4) = 0.212371068288"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(5) = 0.164051880088"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(6) = 0.132107164908"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(7) = 0.109763805648"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(8) = 0.0934032425883"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(9) = 0.0809753813626"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(10) = 0.0712508019553"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(11) = 0.0634545876093"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(12) = 0.0570771107138"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(13) = 0.0517709031557"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(14) = 0.0472916574827"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(15) = 0.043463000039"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(16) = 0.0401546547617"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(17) = 0.0372684556085"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(18) = 0.0347291239921"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(19) = 0.0324780307334"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Elapsed time = 31.2784979343 seconds\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Backpropagation\n",
      "\n",
      "The backpropagation algorithm estimates the error for each neuron unit so as to effectively deploy the gradient descent optimisation procedure. It is a popular algorithm, conceptually simple, computationally efficient, and it often works. In order to conduct the estimation of the neuron-wise errors, it first propagates the training data through the network, it then computes the error with the predictions and the target values, and it finally backpropagates the error from the output to the input (generally speaking, from a given layer $Error^{(n)}$ to the immediately former one $Error^{(n-1)}$):\n",
      "\n",
      "$$Error^{(n-1)} = Error^{(n)} \\; \\theta^{(n)}$$\n",
      "\n",
      "Note that the bias neurons don't backpropagate, they are not connected to the former layer.\n",
      "\n",
      "Finally, the gradient is computed so that the weights may be updated. Each weight links an input unit $I$ to an output unit $O$, which also provides the error feedback. The general formula that is derived is shown as folows:\n",
      "\n",
      "$$\\theta^{(t+1)} \\leftarrow \\theta^{(t)} + \\eta \\; I \\; Error \\; O \\; (1 - O)$$\n",
      "\n",
      "From a computational complexity perspective, Backpropagation is much more effective than the numerical gradient applied above because it computes the errors for all the weights in 2 network traversals, whereas numerical gradient needs to compute 2 traversals per parameter."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Iris example with Backprop\n",
      "\n",
      "nn = NeuralNetwork.Multilayer([4,4,3])\n",
      "# Target needs to be divided by 3 because of the sigmoid,\n",
      "# regularisation parameter of 0.2\n",
      "tcost = NeuralNetwork.Cost(nn, iris.data, iris.target/3, 0.2)\n",
      "\n",
      "# Cost value for an untrained network\n",
      "print(\"J(ini) = \" + str(tcost))\n",
      "\n",
      "# Train with numerical gradient, 20 rounds\n",
      "# learning rate is 0.1\n",
      "NeuralNetwork.Backprop(nn, iris.data, iris.target/3, 0.2, 20, 0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "J(ini) = 0.748432444836\n",
        "J(0) = 0.03530109147"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(1) = 0.0161558082296"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(2) = 0.0103082099186"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(3) = 0.0075248179143"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(4) = 0.00590843446177"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(5) = 0.00485610916808"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(6) = 0.00411804374251"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(7) = 0.00357252407443"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(8) = 0.00315330992458"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(9) = 0.00282132530988"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(10) = 0.00255205639773"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(11) = 0.00232935973546"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(12) = 0.00214217773607"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(13) = 0.00198268605948"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(14) = 0.00184519469069"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(15) = 0.00172546807067"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(16) = 0.00162028918211"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(17) = 0.00152717133122"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(18) = 0.0014441624233"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(19) = 0.00136970889556"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Elapsed time = 10.8420958519 seconds\n"
       ]
      }
     ],
     "prompt_number": 5
    }
   ],
   "metadata": {}
  }
 ]
}