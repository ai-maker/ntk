{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Multilayer Perceptron\n",
      "\n",
      "Artificial Neural Networks are powerful models in Artificial Intelligence and Machine Learning because they are suitable for many scenarios. Their elementary working form is direct and simple. However, the devil is in the details, and these models are particularly in need of much empirical expertise to get tuned adequately so as to succeed in solving the problems at hand. \n",
      "\n",
      "The neural network is plausibly renown to be the universal learning system. Without loss of generality, this notebook makes some decisions regarding the model shape/topology, the training method, and the like. These design choices, though, are easily tweaked so that the same implementation may be suitable to solve all kinds of problems. This is accomplished by first breaking down its complexity, and then by depicting a procedure to tackle problems systematically in order to quickly detect model flaws and fix them as soon as possible. Let\u2019s say that the gist of this process is to achieve a \u201clean adaptation\u201d procedure for neural networks.\n",
      "\n",
      "## Theory of Operation\n",
      "\n",
      "Artificial Neural Networks (ANNs) are interesting models in Artificial Intelligence and Machine Learning because they are powerful enough to succeed at solving many different problems. Historical evidence of their importance can be found as most leading technical books dedicate many pages to cover them comprehensibly.\n",
      "\n",
      "Overall, ANNs are general-purpose universal learners driven by data. They conform to the connectionist learning approach, which is based on an interconnected network of simple units. Such simple units, aka neurons, compute a nonlinear function over the weighted sum of their inputs. Neural networks are expressive enough to fit to any dataset at hand, and yet they are flexible enough to generalise their performance to new unseen data. It is true, though, that neural networks are fraught with experimental details and experience makes the difference between a successful model and a skewed one.\n",
      "\n",
      "## Framework\n",
      "\n",
      "Some say that 9 out of 10 people who use neural networks apply a Multilayer Perceptron (MLP). A MLP is basically a feed-forward network with 3 layers (at least): an input layer, an output layer, and a hidden layer in between. Thus, the MLP has no structural loops: information always flows from left (input)to right (output). The lack of inherent feedback saves a lot of headaches. Its analysis is totally straightforward given that the output of the network is always a function of the input, it does not depend on any former state of the model or previous input.\n",
      "\n",
      "![Framework of a multilayer perceptron. Its behaviour is defined by the weight of its connections, which is given by the values of its parameters, i.e., the thetas.](https://github.com/atrilla/ntk/tree/master/explore/images/mlp_framework.png)\n",
      "\n",
      "Regarding the topology of a MLP it is normally assumed to be a densely-meshed one-to-many link model between the layers. This is mathematically represented by two matrices of parameters named \u201cthe thetas\u201d. In any case, if a certain connection is of little relevance with respect to the observable training data, the network will automatically pay little attention to its contribution and assign it a low weight close to zero.\n",
      "\n",
      "## Prediction\n",
      "\n",
      "The evaluation of the output of a MLP, i.e., its prediction, given an input vector of data is a matter of matrix multiplication. To that end, the following variables are described for convenience:\n",
      "* $N$ is the dimension of the input layer.\n",
      "* $H$ is the dimension of the hidden layer.\n",
      "* $K$ is the dimension of the output layer.\n",
      "* $M$ is the dimension of the corpus (number of examples).\n",
      "\n",
      "Given the variables above, the parameters of the network, i.e., the thetas matrices, are defined as follows:\n",
      "* $\\theta^{(IN)} \\rightarrow H \\times (N+1)$\n",
      "* $\\theta^{(OUT)} \\rightarrow K \\times (H+1)$\n",
      "\n",
      "What follows are the ordered steps that need to be followed in order to evaluate the network prediction.\n",
      "\n",
      "### Input Feature Expansion\n",
      "\n",
      "The first step to attain a successful operation of the neural network is to add a bias term to the input feature space (mapped to the input layer):\n",
      "\n",
      "$$a^{(IN)} = [1;\\ x]$$\n",
      "\n",
      "The feature expansion of the input space with the bias term increases the learning effectiveness of the model because it adds a degree of freedom to the adaptation process. Note that $a^{(IN)}$ directly represents the activation values of the input layer. Thus, the input layer is linear with the input vector $x$ (it is defined by a linear activation function).\n",
      "\n",
      "### Transit to the Hidden Layer\n",
      "\n",
      "Once the activations (outputs) of the input layer are determined, their values flow into the hidden layer through the weights defined in $\\theta^{(IN)}$:\n",
      "\n",
      "$$z^{(HID)} = \\theta^{(IN)}\\;a^{(IN)}$$\n",
      "\n",
      "Similarly, the dimensionality of the hidden layer is expanded with a bias term to increase its learning effectiveness:\n",
      "\n",
      "$$a^{(HID)} = [1;\\ g(z^{(HID)})]$$\n",
      "\n",
      "Here, a new function $g()$ is introduced. This is the generic activation function of a neuron, and generally it is non-linear. Its application yields the output values of the hidden layer $a^{(HID)}$ and provides the true learning power to the neural model.\n",
      "\n",
      "### Output Prediction\n",
      "\n",
      "Finally, the activation values of the output layer, i.e., the network prediction, are calculated as follows:\n",
      "\n",
      "$$z^{(OUT)} = \\theta^{(OUT)}\\;a^{(HID)}$$\n",
      "\n",
      "$$a^{(OUT)} = g(z^{(OUT)}) = y$$\n",
      "\n",
      "### Activation Function\n",
      "\n",
      "The activation function of the neuron is (usually) a non-linear function that provides the expressive power to the neural network. It is recommended this function to be smooth, differentiable and monotonically non-decreasing (for learning purposes). Typically, the sigmoid function is used.\n",
      "\n",
      "$$g(z) = \\frac{1}{(1 + \\exp^{-z})}$$\n",
      "\n",
      "Note that the range of this function varies from 0 to 1. Therefore, the output values of the neurons will always be bounded by the upper and the lower limits of this range. This entails considering a scaling process if a broader range of predicted values is needed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}