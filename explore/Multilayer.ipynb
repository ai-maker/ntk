{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Multilayer Perceptron\n",
      "\n",
      "Some say that 9 out of 10 people who use neural networks apply a Multilayer Perceptron (MLP). A MLP is basically a feed-forward network with 3 layers (at least): an input layer, an output layer, and a hidden layer in between. Thus, the MLP has no structural loops: information always flows from left (input)to right (output). The lack of inherent feedback saves a lot of headaches. Its analysis is totally straightforward given that the output of the network is always a function of the input, it does not depend on any former state of the model or previous input.\n",
      "\n",
      "![Multilayer Perceptron topology](files/multilayer.png)\n",
      "\n",
      "Regarding the topology of a MLP it is normally assumed to be a densely-meshed one-to-many link model between the layers. This is mathematically represented by two matrices of parameters named \u201cthe thetas\u201d. In any case, if a certain connection is of little relevance with respect to the observable training data, the network will automatically pay little attention to its contribution and assign it a low weight close to zero.\n",
      "\n",
      "## Prediction\n",
      "\n",
      "The evaluation of the output of a MLP, i.e., its prediction, given an input vector of data is a matter of matrix multiplication. To that end, the following variables are described for convenience:\n",
      "* $N$ is the dimension of the input layer.\n",
      "* $H$ is the dimension of the hidden layer.\n",
      "* $K$ is the dimension of the output layer.\n",
      "* $M$ is the dimension of the corpus (number of examples).\n",
      "\n",
      "Given the variables above, the parameters of the network, i.e., the thetas matrices, are defined as follows:\n",
      "* $\\theta^{(IN)} \\rightarrow H \\times (N+1)$\n",
      "* $\\theta^{(OUT)} \\rightarrow K \\times (H+1)$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import NeuralNetwork\n",
      "\n",
      "# 2 input neurons, 3 hidden neurons, 1 output neuron\n",
      "nn = NeuralNetwork.Multilayer([2,3,1])\n",
      "\n",
      "# nn[0] -> ThetaIN, nn[1] -> ThetaOUT\n",
      "print(nn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[array([[-0.77879645,  0.35117581, -0.8748505 ],\n",
        "       [ 0.42304497, -0.94409569, -0.55999281],\n",
        "       [-0.54529225, -0.08765094,  0.33527394]]), array([[-0.01227896,  0.10041227,  0.82567863,  0.90636699]])]\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What follows are the ordered steps that need to be followed in order to evaluate the network prediction.\n",
      "\n",
      "### Input Feature Expansion\n",
      "\n",
      "The first step to attain a successful operation of the neural network is to add a bias term to the input feature space (mapped to the input layer):\n",
      "\n",
      "$$a^{(IN)} = [1;\\ x]$$\n",
      "\n",
      "The feature expansion of the input space with the bias term increases the learning effectiveness of the model because it adds a degree of freedom to the adaptation process. Note that $a^{(IN)}$ directly represents the activation values of the input layer. Thus, the input layer is linear with the input vector $x$ (it is defined by a linear activation function).\n",
      "\n",
      "### Transit to the Hidden Layer\n",
      "\n",
      "Once the activations (outputs) of the input layer are determined, their values flow into the hidden layer through the weights defined in $\\theta^{(IN)}$:\n",
      "\n",
      "$$z^{(HID)} = \\theta^{(IN)}\\;a^{(IN)}$$\n",
      "\n",
      "Similarly, the dimensionality of the hidden layer is expanded with a bias term to increase its learning effectiveness:\n",
      "\n",
      "$$a^{(HID)} = [1;\\ g(z^{(HID)})]$$\n",
      "\n",
      "Here, a new function $g()$ is introduced. This is the generic activation function of a neuron, and generally it is non-linear. Its application yields the output values of the hidden layer $a^{(HID)}$ and provides the true learning power to the neural model.\n",
      "\n",
      "### Output\n",
      "\n",
      "Then, the activation values of the output layer, i.e., the network prediction, are calculated as follows:\n",
      "\n",
      "$$z^{(OUT)} = \\theta^{(OUT)}\\;a^{(HID)}$$\n",
      "\n",
      "and finally\n",
      "\n",
      "$$a^{(OUT)} = g(z^{(OUT)}) = y$$\n",
      "\n",
      "### Activation Function\n",
      "\n",
      "The activation function of the neuron is (usually) a non-linear function that provides the expressive power to the neural network. It is recommended this function to be smooth, differentiable and monotonically non-decreasing (for learning purposes). Typically, the logistic sigmoid function is used.\n",
      "\n",
      "$$g(z) = \\frac{1}{(1 + \\exp^{-z})}$$\n",
      "\n",
      "Note that the range of this function varies from 0 to 1. Therefore, the output values of the neurons will always be bounded by the upper and the lower limits of this range. This entails considering a scaling process if a broader range of predicted values is needed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "# Random instance with 2 values\n",
      "x = np.array([1.0, 2.0])\n",
      "y = NeuralNetwork.Predict(nn, x)\n",
      "\n",
      "# intermediate results are available\n",
      "# y[0] -> input result, y[1] -> hidden result, y[2] -> output result\n",
      "print(y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[array([ 1.,  2.]), array([ 0.10180558,  0.1623241 ,  0.50940006]), array([ 0.6442078])]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "z = np.arange(-8, 8, 0.1)\n",
      "g = NeuralNetwork.Sigmoid(z)\n",
      "\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(z, g, 'b-', label='g(z)')\n",
      "plt.legend(loc='upper left')\n",
      "plt.xlabel('Input [z]')\n",
      "plt.ylabel('Output [g]')\n",
      "plt.title('Logistic sigmoid activation function')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEVCAYAAADn6Y5lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f7f16ba3610>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Training\n",
      "\n",
      "Training a neural network essentially means fitting its parameters to a set of example data considering an objective function, aka cost function. This process is also known as supervised learning. It is usually implemented as an iterative procedure.\n",
      "\n",
      "### Cost Function\n",
      "\n",
      "The cost function somehow encodes the objective or goal that should be attained with the network. It is usually defined as a classification or a regression evaluation function. However, the actual form of the cost function is effectively the same, which is an error or fitting function.\n",
      "\n",
      "The cost function $J$ quantifies the amount of squared error (or misfitting) that the network displays with respect to a set of data. Thus, in order to achieve a successfully working model, this cost function must be minimised with an adequate set of parameter values. To do so, several solutions are valid as long as this cost function be a convex function (i.e., a bowl-like shape). A well known example of such is the quadratic function, which trains the neural network considering a minimum squared error criterion over the whole dataset of training examples:\n",
      "\n",
      "$$J(\\theta, x) = \\frac{1}{M} \\sum_{m=1}^M \\sum_{k=1}^K \\left(Error_k^{(m)}\\right)^2 = \\frac{1}{M} \\sum_{m=1}^M \\sum_{k=1}^K \\left(t_k^{(m)}-y_k^{(m)}\\right)^2$$\n",
      "\n",
      "Note that the term $t$ in the cost function represents the target value of the network (i.e., the ideal/desired network output) for a given input data value $x$. Now that the cost function can be expressed, a convex optimisation procedure (e.g., a gradient-based method) must be conducted in order to minimise its value. Note that this is essentially a least-squares regression.\n",
      "\n",
      "### Regularisation\n",
      "\n",
      "The mean squared-error cost function described above does not incorporate any knowledge or constraint about the characteristics of the parameters being adjusted through the optimisation training strategy. This may develop into a generalisation problem because the space of solutions is large and some of these solutions may turn the model unstable with new unseen data. Therefore, there is the need to smooth the performance of the model over a wide range of input data.\n",
      "\n",
      "Neural networks usually generalise well as long as the weights are kept small. Thus, the Tikhonov regularisation process, aka ridge regression, is introduced as a means to control complexity of the model in favour of its increased general performance. This regularisation approach favours small weight values (it is a cost over large weight values):\n",
      "\n",
      "$$R(\\theta) = \\frac{\\lambda}{2 M} \\sum_{\\forall \\theta \\notin bias} \\theta^2$$\n",
      "\n",
      "There is a typical trade-off in Machine Learning, known as the bias-variance trade-off, which has a direct relationship with the complexity of the model, the nature of the data and the amount of available training data to adjust it. This ability of the model to learn more or less complex scenarios raises an issue with respect to its fitting: if the data is simple to explain, a complex model is said to overfit the data, causing its overall performance to drop (high variance model). Similarly, if complex data is tackled with a simple model, such model is said to underfit the data, also causing its overall performance to drop (high bias model). As it is usual in engineering, a compromise must be reached with an adequate \u03bb\\lambda\u03bb value.\n",
      "\n",
      "### Parameter Initialisation\n",
      "\n",
      "The initial weights of the thetas assigned by the training process are critical with respect to the success of the learning strategy. They determine the starting point of the optimisation procedure, and depending on their value, the adjusted parameter values may end up in different places if the cost function has multiple (local) minima.\n",
      "\n",
      "The parameter initialisation process is based on a uniform distribution between two small numbers that take into account the amount of input and output units of the adjacent layers:\n",
      "\n",
      "$$\\theta_{init} = U[-\\sigma, +\\sigma]\\ \\ where\\ \\ \\sigma = \\frac{\\sqrt{6}}{\\sqrt{in + out}}$$\n",
      "\n",
      "In order to ensure a proper learning procedure, the weights of the parameters need to be randomly assigned in order to prevent any symmetry in the topology of the network model (that would be likely to end in convergence problems).\n",
      "\n",
      "### Gradient Descent\n",
      "\n",
      "Given the convex shape of the cost function (which usually also includes the regularisation), the minimisation objective boils down to finding the extremum of this function using its derivative in the continuos space of the weights. To this end you may use the analytic form of the derivative of the cost function (a nightmare), a numerical finite difference, or automatic differentiation.\n",
      "\n",
      "Gradient descent is a first-order optimisation algorithm, complete but non-optimal. It first starts with some arbitrarily chosen parameters and computes the derivative of the cost function with respect to each of them $\\frac{\\partial J(\\theta,x)}{\\partial \\theta}$. The model parameters are then updated by moving them some distance (determined by the so called learning rate $\\eta$) from the former initial point in the direction of the steepest descent, i.e., along the negative of the gradient. If $\\eta$ is set too small, though, convergence is needlessly slow, whereas if it is too large, the update correction process may overshoot and even diverge.\n",
      "\n",
      "$$\\theta^{t+1} \\leftarrow \\theta^t - \\eta \\frac{\\partial^t J(\\theta,x)}{\\partial \\theta} $$\n",
      "\n",
      "These steps are iterated in a loop until some stopping criterion is met, e.g., a determined number of epochs (i.e., the single presentation of all patterns in the training example set) is reached. One last remark should be made about the amount of examples $M$ used for learning. If the training procedure considers several instances at once per cost computation and parameter update, i.e., $M \\gg 1$, the approach is called batch learning. Batch learning is slow because each cost computation accounts for all the available training instances. In contrast, it is usual to consider only one training instance at a time, i.e., $M=1$, in order to speed up the iterative learning process. This procedure is called online learning. Online learning steps are faster to compute, but this single-instance approximation of the cost function makes it a little inaccurate around the optimum. However, online learning is very convenient in most cases."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load Iris dataset, 4 input, 1 output\n",
      "from sklearn import datasets as dset\n",
      "\n",
      "iris = dset.load_iris()\n",
      "\n",
      "nn = NeuralNetwork.Multilayer([4,4,3])\n",
      "# Target needs to be divided by 3 because of the sigmoid,\n",
      "# regularisation parameter of 0.2\n",
      "tcost = NeuralNetwork.Cost(nn, iris.data, iris.target/2.0, 0.2)\n",
      "\n",
      "# Cost value for an untrained network\n",
      "print(\"J(ini) = \" + str(tcost))\n",
      "\n",
      "# Train with numerical gradient, 20 rounds\n",
      "# learning rate is 0.1\n",
      "NeuralNetwork.NumGradDesc(nn, iris.data, iris.target/2.0, 0.2, 20, 0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "J(ini) = 0.624640084707\n",
        "J(0) = 0.603004234074"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(1) = 0.579925022866"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(2) = 0.555662679625"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(3) = 0.535765647231"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(4) = 0.524090236076"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(5) = 0.517668694861"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(6) = 0.51373081438"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(7) = 0.511064168044"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(8) = 0.509136273119"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(9) = 0.507681570517"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(10) = 0.506551151644"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(11) = 0.505653595852"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(12) = 0.504928870116"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(13) = 0.504335551052"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(14) = 0.503843989412"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(15) = 0.503432372632"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(16) = 0.50308431372"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(17) = 0.502787298845"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(18) = 0.502531646179"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(19) = 0.502309784125"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Elapsed time = 3.1147840023 seconds\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Backpropagation\n",
      "\n",
      "The backpropagation algorithm estimates the error for each neuron unit so as to effectively deploy the gradient descent optimisation procedure. It is a popular algorithm, conceptually simple, computationally efficient, and it often works. In order to conduct the estimation of the neuron-wise errors, it first propagates the training data through the network, it then computes the error with the predictions and the target values, and it finally backpropagates the error from the output to the input, generally speaking, from a given layer $(n)$ to the immediately former one $(n-1)$:\n",
      "\n",
      "$$Error^{(n-1)} = Error^{(n)} \\; \\theta^{(n)}$$\n",
      "\n",
      "Note that the bias neurons don't backpropagate, they are not connected to the former layer.\n",
      "\n",
      "Finally, the gradient is computed so that the weights may be updated. Each weight links an input unit $I$ to an output unit $O$, which also provides the error feedback. The general formula that is derived is shown as folows:\n",
      "\n",
      "$$\\theta^{(t+1)} \\leftarrow \\theta^{(t)} + \\eta \\; I \\; Error \\; O \\; (1 - O)$$\n",
      "\n",
      "From a computational complexity perspective, Backpropagation is much more effective than the numerical gradient applied above because it computes the errors for all the weights in 2 network traversals, whereas numerical gradient needs to compute 2 traversals per parameter. In addition, it also converges much faster."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Iris example with Backprop\n",
      "\n",
      "nn = NeuralNetwork.Multilayer([4,4,3])\n",
      "# Target needs to be divided by 2 because of the sigmoid, values 0, 0.5, 1,\n",
      "# regularisation parameter of 0.2\n",
      "tcost = NeuralNetwork.Cost(nn, iris.data, iris.target/2.0, 0.2)\n",
      "\n",
      "# Cost value for an untrained network\n",
      "print(\"J(ini) = \" + str(tcost))\n",
      "\n",
      "# Train with numerical gradient, 20 rounds\n",
      "# learning rate is 0.1\n",
      "NeuralNetwork.Backprop(nn, iris.data, iris.target/2.0, 0.2, 20, 0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "J(ini) = 0.612099418133\n",
        "J(0) = 0.565994381154\n",
        "J(1) = 0.510986180551"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(2) = 0.396402383484\n",
        "J(3) = 0.293576755578"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(4) = 0.231917422483\n",
        "J(5) = 0.199241812846"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(6) = 0.180685443109\n",
        "J(7) = 0.174911780519"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(8) = 0.171087470744\n",
        "J(9) = 0.16820095705"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(10) = 0.167373396974\n",
        "J(11) = 0.168122359653"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(12) = 0.169524400053\n",
        "J(13) = 0.170660330537"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(14) = 0.171614633694\n",
        "J(15) = 0.172427977614"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(16) = 0.173099025628\n",
        "J(17) = 0.173620810448"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(18) = 0.173974040182\n",
        "J(19) = 0.174134023616"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Elapsed time = 0.843034982681 seconds\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Practical Techniques\n",
      "\n",
      "The convergence of Backpropagation learning can sometimes be tricky. Designing and training a MLP using Backprop requires making choices such as the number and type of nodes, layers, learning rates, training and test sets, etc, and many undesirable behaviours can be avoided with practical techniques.\n",
      "\n",
      "#### Instance Shuffling"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.utils import shuffle\n",
      "import copy\n",
      "\n",
      "idat, itar = iris.data, iris.target/2.0\n",
      "\n",
      "# both networks will start learning with the same weight set\n",
      "nn = NeuralNetwork.Multilayer([4,4,3])\n",
      "nn2 = copy.deepcopy(nn)\n",
      "\n",
      "print(\"Network 1 (instances sequentially ordered by class)\")\n",
      "NeuralNetwork.Backprop(nn, idat, itar, 0.2, 20, 0.1)\n",
      "\n",
      "print(\"\")\n",
      "\n",
      "# shuffle instances\n",
      "idat, itar = shuffle(idat, itar)\n",
      "\n",
      "print(\"Network 2 (shuffled instances)\")\n",
      "NeuralNetwork.Backprop(nn2, idat, itar, 0.2, 20, 0.1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Network 1 (instances sequentially ordered by class)\n",
        "J(0) = 0.621321750106"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(1) = 0.587913543853"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(2) = 0.463654122497"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(3) = 0.328476364559\n",
        "J(4) = 0.246931516909"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(5) = 0.206768733538\n",
        "J(6) = 0.18740759448"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(7) = 0.179427200282\n",
        "J(8) = 0.184798691841"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(9) = 0.185598621558\n",
        "J(10) = 0.184121367158"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(11) = 0.182241429435\n",
        "J(12) = 0.180566162308"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(13) = 0.179121426668\n",
        "J(14) = 0.177861132071"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(15) = 0.176719200868\n",
        "J(16) = 0.175637280254"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(17) = 0.174570139245\n",
        "J(18) = 0.173489598545"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(19) = 0.17237761315\n",
        "Elapsed time = 0.916344881058 seconds\n",
        "\n",
        "Network 2 (shuffled instances)\n",
        "J(0) = 0.386832989653"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(1) = 0.253446172204\n",
        "J(2) = 0.17593457603"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(3) = 0.13437980147\n",
        "J(4) = 0.110683243349"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(5) = 0.0976614577619\n",
        "J(6) = 0.0914164000755"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(7) = 0.0891141123506\n",
        "J(8) = 0.088701359796"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(9) = 0.0879097326682\n",
        "J(10) = 0.0745110096887"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(11) = 0.064894075676\n",
        "J(12) = 0.0600249284325"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(13) = 0.055882049192\n",
        "J(14) = 0.0524687728982"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(15) = 0.0494789388565\n",
        "J(16) = 0.0468512301668"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(17) = 0.0445547667453\n",
        "J(18) = 0.0425300365226"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "J(19) = 0.0407112076363\n",
        "Elapsed time = 0.838934183121 seconds\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notes:\n",
      "\n",
      "The success of Artificial Intelligence and Machine Learning applications is plausibly conceived as a matter of controlling several key variables and following a series of \u201cgood practices\u201d. There are some works in the literature that identify the bits and pieces to be taken into account when designing a successful model. Some of these are described as follows:\n",
      "\n",
      "    Focus on model generalisation: keep a separate self-validation set of data (not used to train the model) to test and estimate the actual performance of the model.\n",
      "    Incorporate as much knowledge as possible. Expertise is a key indicator of success. Data driven models don\u2019t do magic, the more information that is available, the greater the performance of the model.\n",
      "    Feature Engineering is of utmost importance. This relates to the former point: the more useful information that can be extracted from the input data, the better performance can be expected. Salient indicators are keys to success. This may lead to selecting only the most informative features (mutual information, chi-square\u2026), or to change the feature space that is used to represent the instance data (Principal Component Analysis\u2026). And always standardise your data and exclude outliers.\n",
      "    Get more data if the model is not good enough. Related to \u201cthe curse of dimensionality\u201d principle: if good data is lacking, no successful model can be obtained. There must be a coherent relation between the parameters of the model (i.e., its complexity) and the amount of available data to train them.\n",
      "    Ensemble models, integrate criteria. Bearing in mind that the optimum model structure is not known in advance, one of the most reasonable approaches to obtain a fairly good guess is to apply different models (with different learning features) to the same problem and combine/weight their outputs. Related techniques to this are also known as \u201cboosting\u201d.\n",
      "\n",
      "The following sections delve into some of these topics with some practical strategies.\n",
      "\n",
      "Target Values\n",
      "\n",
      "When designing a learning system, it is suitable to take into account the nature of the problem at hand (e.g., whether if it is a classification problem or a regression problem) to determine the number of output units KKK.\n",
      "\n",
      "In the case of classification, KKK should be the amount of different classes, and the target output should be a binary vector. Given an instance, only the output unit that corresponds to the instance class should be set. The decision rule for classification is then driven by the maximum output unit. \n",
      "\n",
      "In the case of a regression problem, KKK should be equal to the number of dependent variables. \n",
      "\n",
      "Hidden Units\n",
      "\n",
      "The number of hidden units determines the expressive power of the network, and thus, the complexity of its transfer function. The more complex a model is, the more complicated data structures it can learn. Nevertheless, this argument cannot be extended ad infinitum because a shortage of training data with respect to the amount of parameters to be learnt may lead the model to overfit the data. That\u2019s why the aforementioned regularisation function is used to avoid this situation.\n",
      "\n",
      "Thus, it is common to have a skew toward suggesting a slightly more complex model than strictly necessary (regularisation will compensate for the extra complexity if necessary). Some heuristic guidelines to guess this optimum number of hidden units indicate an amount somewhat related to the number of input and output units. This is an experimental issue, though. There is no rule of thumb for this. Apply a configuration that works for your problem and you\u2019re done.\n",
      "\n",
      "Deep learning -> many layers, vanishing gradient, need to train differently"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}